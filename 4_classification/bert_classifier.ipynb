{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yNa_d21ca8yO",
        "rzi8I76sbXXd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "Sentiment analysis using Bidirectional Encoder Representations from Transformers classifier\n",
        "* BERT model\n",
        "* RoBERTa model"
      ],
      "metadata": {
        "id": "vKPo1s32ass6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and install required libraries"
      ],
      "metadata": {
        "id": "yNa_d21ca8yO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3547f4af"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt \n",
        "sb.set() \n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertTokenizer,BertForSequenceClassification\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read dataset"
      ],
      "metadata": {
        "id": "rzi8I76sbXXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1054027"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('../dataset/train_aug.csv')\n",
        "test_data = pd.read_csv('../dataset/test.csv')\n",
        "full_dataset_df = pd.read_csv('../dataset/full_dataset_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT model"
      ],
      "metadata": {
        "id": "zzVc5ey2cori"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a9c73e0"
      },
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d2be1ed"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.drop = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ,return_dict=False)\n",
        "        \n",
        "        output = self.drop(pooled_output)\n",
        "    \n",
        "        \n",
        "#         print(input_ids.shape)\n",
        "#         print(input_ids.dtype)\n",
        "#         print(attention_mask.shape)\n",
        "#         print(attention_mask.dtype)\n",
        "#         print(type(pooled_output))\n",
        "#         print(pooled_output)\n",
        "        \n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4882c1ad",
        "outputId": "a5b8e2b6-f800-4e65-c98c-46b304e43ef9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = SentimentClassifier(n_classes=3, ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d27a2240"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b70396ad"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d02c08b6"
      },
      "outputs": [],
      "source": [
        "X_train, x_val, Y_train, y_val = train_test_split(train_data['clean_text'], train_data['manual_label'], test_size=0.2,stratify=train_data['manual_label'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d00a4385"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = test_data['clean_text'].to_numpy(), test_data['manual_label'].to_numpy()\n",
        "X_full_dataset, Y_full_dataset = full_dataset_df['clean_text'].to_numpy(), [0]*full_dataset_df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a02f759e",
        "outputId": "0d8f0b44-9921-4838-e820-823616dd4194"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/UG/teog0015/.conda/envs/patenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text and convert them to PyTorch tensors\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.values.tolist(),\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "val_tokens = tokenizer.batch_encode_plus(\n",
        "    x_val.values.tolist(),\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test,\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "\n",
        "full_dataset_tokens = tokenizer.batch_encode_plus(\n",
        "    X_full_dataset,\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be90e00f",
        "outputId": "575df536-4237-4e10-e43e-a1e26b6a087e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/UG/teog0015/.conda/envs/patenv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Encode the labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "train_y_encoded = label_encoder.fit_transform(Y_train)\n",
        "val_y_encoded = label_encoder.transform(y_val)\n",
        "test_y_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# One-hot encode the labels\n",
        "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "train_y = onehot_encoder.fit_transform(train_y_encoded.reshape(-1, 1))\n",
        "val_y = onehot_encoder.transform(val_y_encoded.reshape(-1, 1))\n",
        "test_y = onehot_encoder.transform(test_y_encoded.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57848354"
      },
      "outputs": [],
      "source": [
        "train_seq = torch.tensor(train_tokens['input_ids'])\n",
        "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
        "train_y = torch.tensor(train_y)\n",
        "\n",
        "val_seq = torch.tensor(val_tokens['input_ids'])\n",
        "val_mask = torch.tensor(val_tokens['attention_mask'])\n",
        "val_y = torch.tensor(val_y)\n",
        "\n",
        "test_seq = torch.tensor(test_tokens['input_ids'])\n",
        "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
        "test_y = torch.tensor(test_y)\n",
        "\n",
        "full_dataset_seq = torch.tensor(full_dataset_tokens['input_ids'])\n",
        "full_dataset_mask = torch.tensor(full_dataset_tokens['attention_mask'])\n",
        "full_dataset_y = torch.tensor(Y_full_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "643e9b78"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_dataloader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ce15db4"
      },
      "outputs": [],
      "source": [
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_dataloader = DataLoader(val_data, batch_size=VALID_BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cda96b3"
      },
      "outputs": [],
      "source": [
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "test_dataloader = DataLoader(test_data, batch_size=VALID_BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "681dc050"
      },
      "outputs": [],
      "source": [
        "full_data = TensorDataset(full_dataset_seq, full_dataset_mask, full_dataset_y)\n",
        "full_dataset_dataloader = DataLoader(full_data, batch_size=VALID_BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "653fc2a8",
        "outputId": "fd4c6ade-a7ee-4c52-c8c6-ad3b120fcc12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:25<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.7804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 24.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.4958\n",
            "Validation Accuracy: 0.7902\n",
            "Validation Precision: 0.7980\n",
            "Validation Recall: 0.7905\n",
            "Validation F1-score: 0.7909\n",
            "Epoch 2/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:26<00:00,  4.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.4062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 23.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.3012\n",
            "Validation Accuracy: 0.8750\n",
            "Validation Precision: 0.8719\n",
            "Validation Recall: 0.8761\n",
            "Validation F1-score: 0.8710\n",
            "Epoch 3/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:25<00:00,  4.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.2381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 23.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2586\n",
            "Validation Accuracy: 0.8929\n",
            "Validation Precision: 0.9038\n",
            "Validation Recall: 0.8941\n",
            "Validation F1-score: 0.8967\n",
            "Epoch 4/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:25<00:00,  4.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.1641\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 24.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2443\n",
            "Validation Accuracy: 0.9018\n",
            "Validation Precision: 0.9048\n",
            "Validation Recall: 0.9032\n",
            "Validation F1-score: 0.9037\n",
            "Epoch 5/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:25<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.1145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 24.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2771\n",
            "Validation Accuracy: 0.9018\n",
            "Validation Precision: 0.9089\n",
            "Validation Recall: 0.9009\n",
            "Validation F1-score: 0.9027\n",
            "Training time: 141.067\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "\n",
        "\n",
        "print_freq = 20\n",
        "epochs = 5\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    precision_ = 0\n",
        "    recall_ = 0\n",
        "    f1_ = 0\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 20)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        batch_seq, batch_mask, batch_y = tuple(t.to(device) for t in batch)\n",
        "        outputs = model(input_ids= batch_seq, attention_mask=batch_mask)\n",
        "        loss = loss_fn(outputs, batch_y)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    print(f'Training loss: {train_loss:.4f}')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = 0\n",
        "    eval_acc = 0\n",
        "    nb_eval_steps = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader):\n",
        "        batch_seq, batch_mask, batch_y = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch_seq, attention_mask=batch_mask)\n",
        "        logits = outputs\n",
        "        label_ids = batch_y.to('cpu').numpy()\n",
        "        tmp_eval_accuracy = accuracy_score(np.argmax(label_ids, axis=1), np.argmax(logits.detach().cpu().numpy(), axis=1))\n",
        "        eval_acc += tmp_eval_accuracy\n",
        "        predictions.append(np.argmax(logits.detach().cpu(), axis=1))\n",
        "        true_labels.append(np.argmax(label_ids, axis=1))\n",
        "        loss = loss_fn(logits.detach().cpu(), batch_y.to('cpu').argmax(dim=1))\n",
        "        eval_loss += loss.item()\n",
        "        nb_eval_steps += 1\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_acc = eval_acc / len(val_dataloader)\n",
        "    print(f'Validation loss: {eval_loss:.4f}')\n",
        "    print(f'Validation Accuracy: {eval_acc:.4f}')\n",
        "\n",
        "    # Compute precision, recall, and F1-score\n",
        "    predictions = np.concatenate(predictions)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
        "    precision_ += precision\n",
        "    recall_ += recall\n",
        "    f1_ += f1\n",
        "    print(f'Validation Precision: {precision:.4f}')\n",
        "    print(f'Validation Recall: {recall:.4f}')\n",
        "    print(f'Validation F1-score: {f1:.4f}')\n",
        "\n",
        "end = time.time()\n",
        "print(\"Training time: {:.3f}\".format(end-start))\n",
        "torch.save(model.state_dict(), 'BertModel.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6b6cef2",
        "outputId": "d3cad265-29bb-4438-fe6e-57ca5b493fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.4769\n",
            "Test Accuracy: 0.8400\n",
            "Test Precision: 0.8469\n",
            "Test Recall: 0.8400\n",
            "Test F1-score: 0.8425\n",
            "Evaluation time on test dataset: 1.992\n"
          ]
        }
      ],
      "source": [
        "# Test the model on the test set\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_acc = 0\n",
        "nb_test_steps = 0\n",
        "predictions = []\n",
        "true_labels = []\n",
        "start = time.time()\n",
        "for batch in test_dataloader:\n",
        "    batch_seq, batch_mask, batch_y = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_seq, attention_mask=batch_mask)\n",
        "    logits = outputs\n",
        "    label_ids = batch_y.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = accuracy_score(np.argmax(label_ids, axis=1), np.argmax(logits.detach().cpu().numpy(), axis=1))\n",
        "    test_acc += tmp_eval_accuracy\n",
        "    predictions.append(np.argmax(logits.detach().cpu(), axis=1))\n",
        "    true_labels.append(np.argmax(label_ids, axis=1))\n",
        "    loss = torch.nn.functional.cross_entropy(logits.detach().cpu(), batch_y.to('cpu').argmax(dim=1))\n",
        "    test_loss += loss.item()\n",
        "    nb_eval_steps += 1\n",
        "    nb_test_steps +=1\n",
        "\n",
        "end = time.time()\n",
        "test_loss = test_loss / nb_test_steps\n",
        "test_acc = test_acc / len(test_dataloader)\n",
        "print(f'Test loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "test_predictions = np.concatenate(predictions)\n",
        "test_true_labels = np.concatenate(true_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(test_true_labels, test_predictions, average='weighted')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1-score: {f1:.4f}')\n",
        "print(\"Evaluation time on test dataset: {:.3f}\".format(end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5224eba",
        "outputId": "a1771858-a1c3-4225-bfba-f65b4ca68fc9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>predicted_label</th>\n",
              "      <th>actual_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>day mapo tofu healthy meal mean tofu got ta co...</td>\n",
              "      <td>pos</td>\n",
              "      <td>neu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>azuki room magicwin cryptochazman yasirali nft...</td>\n",
              "      <td>neu</td>\n",
              "      <td>neu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nft lending agreement benddao ethereum reserve...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>meebit bought eth usd blur meebits meebitsnft</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sold punksticker new owner thanks enjoy nftcol...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>meebits swept total eth usd blur meebits meebi...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>voxel punkbit new owner sumrtime voxelpunkbits...</td>\n",
              "      <td>neu</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>hmm mekaverse apekidsclub floor almost price n...</td>\n",
              "      <td>neu</td>\n",
              "      <td>neu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>azuki sold eth nft collection azuki floor pric...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>public sale live cybotz nft top nft sale last ...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            clean_text predicted_label  \\\n",
              "0    day mapo tofu healthy meal mean tofu got ta co...             pos   \n",
              "1    azuki room magicwin cryptochazman yasirali nft...             neu   \n",
              "2    nft lending agreement benddao ethereum reserve...             pos   \n",
              "3        meebit bought eth usd blur meebits meebitsnft             pos   \n",
              "4    sold punksticker new owner thanks enjoy nftcol...             pos   \n",
              "..                                                 ...             ...   \n",
              "395  meebits swept total eth usd blur meebits meebi...             pos   \n",
              "396  voxel punkbit new owner sumrtime voxelpunkbits...             neu   \n",
              "397  hmm mekaverse apekidsclub floor almost price n...             neu   \n",
              "398  azuki sold eth nft collection azuki floor pric...             pos   \n",
              "399  public sale live cybotz nft top nft sale last ...             pos   \n",
              "\n",
              "    actual_label  \n",
              "0            neu  \n",
              "1            neu  \n",
              "2            pos  \n",
              "3            pos  \n",
              "4            pos  \n",
              "..           ...  \n",
              "395          pos  \n",
              "396          pos  \n",
              "397          neu  \n",
              "398          pos  \n",
              "399          pos  \n",
              "\n",
              "[400 rows x 3 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the predicted labels back to their original text labels\n",
        "predicted_labels = label_encoder.inverse_transform(test_predictions)\n",
        "actual_labels = label_encoder.inverse_transform(np.argmax(test_y, axis=1))\n",
        "\n",
        "# Create a new DataFrame with the predicted labels and the original labels\n",
        "results_df = pd.DataFrame({'clean_text': X_test, 'predicted_label': predicted_labels, 'actual_label': actual_labels})\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function for displaying the report\n",
        "def Classification_Report(y_true, y_pred):\n",
        "    print(f\"Classification Report:\\n\\n\",classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qudIDJvCCsHj",
        "outputId": "961804d8-b821-4689-e99f-8c302189efc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.67      0.48      0.56        21\n",
            "         neu       0.80      0.70      0.74       102\n",
            "         pos       0.88      0.94      0.91       277\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.78      0.70      0.74       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa model"
      ],
      "metadata": {
        "id": "Jlvi8WoPdYIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPCVbrN9iNnn"
      },
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cd3bc70"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Linear(self.roberta.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        return_dict=False)\n",
        "        \n",
        "        # print(input_ids.shape)\n",
        "        # print(input_ids.dtype)\n",
        "        # print(attention_mask.shape)\n",
        "        # print(attention_mask.dtype)\n",
        "        # print(type(pooled_output))\n",
        "        # print(pooled_output)\n",
        "        \n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "f17cf9e3-e594-4a0c-8e76-519dcee84304",
        "id": "Bm2pgStEiNno"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = SentimentClassifier(n_classes=3).to(device)\n",
        "model= model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKuO7orziNno"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIhBL8quiNno"
      },
      "outputs": [],
      "source": [
        "X_train, x_val, Y_train, y_val = train_test_split(train_data['clean_text'], train_data['manual_label'], test_size=0.2,stratify=train_data['manual_label'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64uijW5UiNno"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = test_data['clean_text'].to_numpy(), test_data['manual_label'].to_numpy()\n",
        "X_full_dataset, Y_full_dataset = full_dataset_df['clean_text'].to_numpy(), [0]*full_dataset_df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "c96c3aed-7770-484e-9682-679ed5588d2f",
        "id": "O0na4OKAiNnp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/UG/teog0015/.conda/envs/patenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text and convert them to PyTorch tensors\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.values.tolist(),\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "val_tokens = tokenizer.batch_encode_plus(\n",
        "    x_val.values.tolist(),\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test,\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "full_dataset_tokens = tokenizer.batch_encode_plus(\n",
        "    X_full_dataset,\n",
        "    max_length=256,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "06fce24b-c5da-42ad-c72a-629eae19941f",
        "id": "f0I7g5PyiNnp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/UG/teog0015/.conda/envs/patenv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Encode the labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "train_y_encoded = label_encoder.fit_transform(Y_train)\n",
        "val_y_encoded = label_encoder.transform(y_val)\n",
        "test_y_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# One-hot encode the labels\n",
        "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "train_y = onehot_encoder.fit_transform(train_y_encoded.reshape(-1, 1))\n",
        "val_y = onehot_encoder.transform(val_y_encoded.reshape(-1, 1))\n",
        "test_y = onehot_encoder.transform(test_y_encoded.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_L7UR4JiNnp"
      },
      "outputs": [],
      "source": [
        "train_seq = torch.tensor(train_tokens['input_ids'])\n",
        "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
        "train_y = torch.tensor(train_y)\n",
        "\n",
        "val_seq = torch.tensor(val_tokens['input_ids'])\n",
        "val_mask = torch.tensor(val_tokens['attention_mask'])\n",
        "val_y = torch.tensor(val_y)\n",
        "\n",
        "test_seq = torch.tensor(test_tokens['input_ids'])\n",
        "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
        "test_y = torch.tensor(test_y)\n",
        "\n",
        "full_dataset_seq = torch.tensor(full_dataset_tokens['input_ids'])\n",
        "full_dataset_mask = torch.tensor(full_dataset_tokens['attention_mask'])\n",
        "full_dataset_y = torch.tensor(Y_full_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M3rgWjaiNnp"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKoUbUGqiNnp"
      },
      "outputs": [],
      "source": [
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, batch_size=VALID_BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNjKdkHAiNnp"
      },
      "outputs": [],
      "source": [
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, batch_size=VALID_BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "592ef291"
      },
      "outputs": [],
      "source": [
        "full_data = TensorDataset(full_dataset_seq, full_dataset_mask, full_dataset_y)\n",
        "full_dataset_dataloader = DataLoader(full_data, batch_size=VALID_BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "ad3a0f10-f577-4f55-9c46-1113c9c61e0c",
        "id": "OMF8CHRiiNnq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:27<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.7964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 22.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.5119\n",
            "Validation Accuracy: 0.7723\n",
            "Validation Precision: 0.7848\n",
            "Validation Recall: 0.7748\n",
            "Validation F1-score: 0.7656\n",
            "Epoch 2/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:28<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.4207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 22.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2960\n",
            "Validation Accuracy: 0.8839\n",
            "Validation Precision: 0.8898\n",
            "Validation Recall: 0.8829\n",
            "Validation F1-score: 0.8845\n",
            "Epoch 3/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:27<00:00,  4.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.2815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 24.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2232\n",
            "Validation Accuracy: 0.9107\n",
            "Validation Precision: 0.9113\n",
            "Validation Recall: 0.9099\n",
            "Validation F1-score: 0.9098\n",
            "Epoch 4/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:26<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.1978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 24.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2554\n",
            "Validation Accuracy: 0.9018\n",
            "Validation Precision: 0.9057\n",
            "Validation Recall: 0.9054\n",
            "Validation F1-score: 0.9048\n",
            "Epoch 5/5\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:26<00:00,  4.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.1337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:02<00:00, 24.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.3536\n",
            "Validation Accuracy: 0.8929\n",
            "Validation Precision: 0.8963\n",
            "Validation Recall: 0.8919\n",
            "Validation F1-score: 0.8811\n",
            "Training time: 146.744\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    precision_ = 0\n",
        "    recall_ = 0\n",
        "    f1_ = 0\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 20)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        batch_seq, batch_mask, batch_y = tuple(t.to(device) for t in batch)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_seq, attention_mask=batch_mask)\n",
        "        loss = loss_fn(outputs, batch_y)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader)\n",
        "    print(f'Training loss: {train_loss:.4f}')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = 0\n",
        "    eval_acc = 0\n",
        "    nb_eval_steps = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader):\n",
        "        batch_seq, batch_mask, batch_y = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch_seq, attention_mask=batch_mask)\n",
        "        logits = outputs\n",
        "        label_ids = batch_y.to('cpu').numpy()\n",
        "        tmp_eval_accuracy = accuracy_score(np.argmax(label_ids, axis=1), np.argmax(logits.detach().cpu().numpy(), axis=1))\n",
        "        eval_acc += tmp_eval_accuracy\n",
        "        predictions.append(np.argmax(logits.detach().cpu(), axis=1))\n",
        "        true_labels.append(np.argmax(label_ids, axis=1))\n",
        "        loss = loss_fn(logits.detach().cpu(), batch_y.to('cpu').argmax(dim=1))\n",
        "        eval_loss += loss.item()\n",
        "        \n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_acc = eval_acc / len(val_dataloader)\n",
        "    print(f'Validation loss: {eval_loss:.4f}')\n",
        "    print(f'Validation Accuracy: {eval_acc:.4f}')\n",
        "\n",
        "    # Compute precision, recall, and F1-score\n",
        "    predictions = np.concatenate(predictions)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
        "    precision_ += precision\n",
        "    recall_ += recall\n",
        "    f1_ += f1\n",
        "    print(f'Validation Precision: {precision:.4f}')\n",
        "    print(f'Validation Recall: {recall:.4f}')\n",
        "    print(f'Validation F1-score: {f1:.4f}')\n",
        "end = time.time()\n",
        "print(\"Training time: {:.3f}\".format(end-start))\n",
        "torch.save(model.state_dict(), 'RoBERTaModel.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "b4656ae9-9271-4ec5-e407-bd66a18c7baf",
        "id": "QyWqhuiDiNnq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5275\n",
            "Test Accuracy: 0.8375\n",
            "Test Precision: 0.8397\n",
            "Test Recall: 0.8375\n",
            "Test F1-score: 0.8295\n",
            "Time taken for evaluation on test data: 1.922\n"
          ]
        }
      ],
      "source": [
        "# Test the model on the test set\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "test_acc = 0\n",
        "nb_test_steps = 0\n",
        "predictions = []\n",
        "true_labels = []\n",
        "start = time.time()\n",
        "for batch in test_dataloader:\n",
        "    batch_seq, batch_mask, batch_y = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_seq, attention_mask=batch_mask)\n",
        "    logits = outputs\n",
        "    label_ids = batch_y.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = accuracy_score(np.argmax(label_ids, axis=1), np.argmax(logits.detach().cpu().numpy(), axis=1))\n",
        "    test_acc += tmp_eval_accuracy\n",
        "    predictions.append(np.argmax(logits.detach().cpu(), axis=1))\n",
        "    true_labels.append(np.argmax(label_ids, axis=1))\n",
        "    loss = torch.nn.functional.cross_entropy(logits.detach().cpu(), batch_y.to('cpu').argmax(dim=1))\n",
        "    test_loss += loss.item()\n",
        "    nb_eval_steps += 1\n",
        "    nb_test_steps +=1\n",
        "\n",
        "end = time.time()\n",
        "test_loss = test_loss / nb_test_steps\n",
        "test_acc = test_acc / len(test_dataloader)\n",
        "print(f'Test loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "test_predictions = np.concatenate(predictions)\n",
        "test_true_labels = np.concatenate(true_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(test_true_labels, test_predictions, average='weighted')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1-score: {f1:.4f}')\n",
        "print(\"Time taken for evaluation on test data: {:.3f}\".format(end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "782fed3b-c236-40b2-c975-d2c8da058066",
        "id": "mGyd5DJ0iNnq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>predicted_label</th>\n",
              "      <th>actual_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>day mapo tofu healthy meal mean tofu got ta co...</td>\n",
              "      <td>pos</td>\n",
              "      <td>neu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>azuki room magicwin cryptochazman yasirali nft...</td>\n",
              "      <td>neu</td>\n",
              "      <td>neu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nft lending agreement benddao ethereum reserve...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>meebit bought eth usd blur meebits meebitsnft</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sold punksticker new owner thanks enjoy nftcol...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>meebits swept total eth usd blur meebits meebi...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>voxel punkbit new owner sumrtime voxelpunkbits...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>hmm mekaverse apekidsclub floor almost price n...</td>\n",
              "      <td>pos</td>\n",
              "      <td>neu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>azuki sold eth nft collection azuki floor pric...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>public sale live cybotz nft top nft sale last ...</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            clean_text predicted_label  \\\n",
              "0    day mapo tofu healthy meal mean tofu got ta co...             pos   \n",
              "1    azuki room magicwin cryptochazman yasirali nft...             neu   \n",
              "2    nft lending agreement benddao ethereum reserve...             pos   \n",
              "3        meebit bought eth usd blur meebits meebitsnft             pos   \n",
              "4    sold punksticker new owner thanks enjoy nftcol...             pos   \n",
              "..                                                 ...             ...   \n",
              "395  meebits swept total eth usd blur meebits meebi...             pos   \n",
              "396  voxel punkbit new owner sumrtime voxelpunkbits...             pos   \n",
              "397  hmm mekaverse apekidsclub floor almost price n...             pos   \n",
              "398  azuki sold eth nft collection azuki floor pric...             pos   \n",
              "399  public sale live cybotz nft top nft sale last ...             pos   \n",
              "\n",
              "    actual_label  \n",
              "0            neu  \n",
              "1            neu  \n",
              "2            pos  \n",
              "3            pos  \n",
              "4            pos  \n",
              "..           ...  \n",
              "395          pos  \n",
              "396          pos  \n",
              "397          neu  \n",
              "398          pos  \n",
              "399          pos  \n",
              "\n",
              "[400 rows x 3 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the predicted labels back to their original text labels\n",
        "predicted_labels = label_encoder.inverse_transform(test_predictions)\n",
        "actual_labels = label_encoder.inverse_transform(np.argmax(test_y, axis=1))\n",
        "\n",
        "# Create a new DataFrame with the predicted labels and the original labels\n",
        "results_df = pd.DataFrame({'clean_text': X_test, 'predicted_label': predicted_labels, 'actual_label': actual_labels})\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function for displaying the report\n",
        "def Classification_Report(y_true, y_pred):\n",
        "    print(f\"Classification Report:\\n\\n\",classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkVwttXWC_7S",
        "outputId": "5911772c-40d9-4648-8c0b-afdbbe44523b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.62      0.86      0.72        21\n",
            "         neu       0.78      0.73      0.75       102\n",
            "         pos       0.91      0.91      0.91       277\n",
            "\n",
            "    accuracy                           0.86       400\n",
            "   macro avg       0.77      0.83      0.79       400\n",
            "weighted avg       0.86      0.86      0.86       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply on full dataset"
      ],
      "metadata": {
        "id": "oaPC3_YDdvey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23cffcb5",
        "outputId": "ef660178-43d6-4051-9f15-20ead386cae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for predictions on full dataset: 100.800\n"
          ]
        }
      ],
      "source": [
        "full_dataset_predictions = []\n",
        "model.eval()\n",
        "start = time.time()\n",
        "for batch in full_dataset_dataloader:\n",
        "    batch_seq, batch_mask, batch_y = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_seq, attention_mask=batch_mask)\n",
        "    logits = outputs\n",
        "    full_dataset_predictions.append(np.argmax(logits.detach().cpu(), axis=1))\n",
        "end = time.time()\n",
        "print(\"Time taken for predictions on full dataset: {:.3f}\".format(end-start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_predictions_concat = np.concatenate(full_dataset_predictions)\n",
        "full_dataset_predicted_labels = label_encoder.inverse_transform(full_dataset_predictions_concat)"
      ],
      "metadata": {
        "id": "Wittuv-SiqnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_df['roberta'] = full_dataset_predicted_labels"
      ],
      "metadata": {
        "id": "tdVsB1XBiquf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = df['roberta'].value_counts()\n",
        "\n",
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZlyDfFrEugl",
        "outputId": "2efe07f4-901b-40dd-a4c5-7833c131ad1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pos    14252\n",
              "neu     4488\n",
              "neg     1555\n",
              "Name: RoBERTa Predictions, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['roberta'].value_counts().plot.bar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "vUbmiAxAEumh",
        "outputId": "fab14c0a-61dd-4ac1-bec3-5b5ad50aca05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_df.to_csv('../dataset/BERT_full_dataset_final.csv', index=False)"
      ],
      "metadata": {
        "id": "AQj5zahoitrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}